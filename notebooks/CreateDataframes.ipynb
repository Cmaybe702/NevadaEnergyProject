{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4178501c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/opt/apache-spark/libexec'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153e3852",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/30 11:41:31 WARN Utils: Your hostname, Courtneys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.6 instead (on interface en0)\n",
      "23/03/30 11:41:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/30 11:41:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.functions import translate, col, month\n",
    "from pyspark.sql.types import DoubleType, TimestampType, StringType\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NV Energy Project\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "sc.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b54e12c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "## ENERGY CONSUMPTION MODEL VS ACTUAL CONSUMPTION\n",
    "#Import consumption data\n",
    "\n",
    "nv_ac_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/nv_annual_consumption60-21.csv\")\n",
    "\n",
    "nv_ac_df = nv_ac_df.withColumn(\"TotalConsumption\", translate(col(\"TotalConsumption\"), \",\", \"\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18b6040",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert data to millions scale for visualization\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "nv_ac_df2 = nv_ac_df.select(\"YEAR\", \"TotalConsumption\", convertUDF(col(\"TotalConsumption\").alias(\"TotalMillions\"))) \n",
    "\n",
    "nv_ac_df2 = nv_ac_df2.withColumnRenamed(\"<lambda>(TotalConsumption AS TotalMillions)\", \"Actual Consumption\")\n",
    "\n",
    "nv_ac_df3 = nv_ac_df2.filter(nv_ac_df2.YEAR != 1990)\n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.filter(nv_ac_df2.YEAR != 2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17c582b",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Get predictions from Consumption LR \n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "convertUDF = udf(lambda z: predict(z), DoubleType())\n",
    "\n",
    "def predict(x):\n",
    "    return  x * 675250.9602239017 - 1325137830.201505\n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.select(\"YEAR\", \"TotalConsumption\", \"Actual Consumption\", \n",
    "                             convertUDF(col(\"YEAR\").alias(\"ModelPred\"))) \n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.withColumnRenamed(\"<lambda>(YEAR AS ModelPred)\", \"ModelPred\")\n",
    "\n",
    "#Convert predictions to millions scale\n",
    "\n",
    "convertUDF2 = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "nv_ac_df4 = nv_ac_df3.select(\"YEAR\", \"TotalConsumption\", \"Actual Consumption\", \"ModelPred\",\n",
    "                             convertUDF2(col(\"ModelPred\").alias(\"PredMillions\"))) \n",
    "\n",
    "nv_ac_df4 = nv_ac_df4.withColumnRenamed(\"<lambda>(ModelPred AS PredMillions)\", \"Model Predictions\")\n",
    "\n",
    "nv_ac_df4 = nv_ac_df4.orderBy(\"YEAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb608d7",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create consumption panda frame, return to dashboard\n",
    "\n",
    "def createNVCFrame():\n",
    "    \n",
    "    nv_ac_df5 = nv_ac_df4.toPandas()\n",
    "    \n",
    "    return nv_ac_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00465b47",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "## NEVADA ENERGY PRODUCTION BY PRIMARY SOURCE\n",
    "#Import primary source data\n",
    "\n",
    "primary_source_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/GenerationBySource.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf460753",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert data to millions scale for visualization\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    \n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "#primary_source_df2 = primary_source_df.filter(primary_source_df.energySource != \"Total\")\n",
    "\n",
    "primary_source_df2 = primary_source_df.select(\"YEAR\", \"EnergySource\", convertUDF(col(\"GenerationMWh\").alias(\"TotalMillions\"))) \n",
    "\n",
    "#Order by year and rename custom column\n",
    "primary_source_df2 = primary_source_df2.orderBy(\"YEAR\")\n",
    "\n",
    "primary_source_df2 = primary_source_df2.withColumnRenamed(\"<lambda>(GenerationMWh AS TotalMillions)\", \"GenerationMil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7914411c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create primary source pandas frame for dashboard\n",
    "\n",
    "def createPSFrame():\n",
    "    ps_df = primary_source_df2.toPandas()\n",
    "\n",
    "    ps_df_grouped = ps_df.groupby(\"EnergySource\")\n",
    "\n",
    "    return ps_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8849729",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "##SUPPLY AND DEMAND\n",
    "#Dataframe for interactive demand\n",
    "\n",
    "hist_demand_df = nv_ac_df.filter(nv_ac_df.YEAR >= 1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90cd83a4",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create demand prediction frame\n",
    "\n",
    "pred_df = spark.range(2022,2050).withColumnRenamed(\"id\",\"YEAR\")\n",
    "\n",
    "convertUDF = udf(lambda z: predict(z), DoubleType())\n",
    "\n",
    "def predict(x):\n",
    "    return  x * 675250.9602239017 - 1325137830.201505\n",
    "\n",
    "pred_df2 = pred_df.select(\"YEAR\", convertUDF(col(\"YEAR\").alias(\"ModelPred\"))) \n",
    "\n",
    "pred_df2 = pred_df2.withColumnRenamed(\"<lambda>(YEAR AS ModelPred\", \"Demand\")\n",
    "\n",
    "#Merge actual and predictions\n",
    "\n",
    "demand_df = hist_demand_df.union(pred_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4cecbc",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert demand to millions scale\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    \n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "demand_df2 = demand_df.select(\"YEAR\", convertUDF(col(\"TotalConsumption\").alias(\"Demand\")))\n",
    "\n",
    "demand_df2 = demand_df2.withColumnRenamed(\"<lambda>(TotalConsumption AS Demand)\", \"Demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aca818f",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "#Dataframe for interactive supply\n",
    "\n",
    "#Historical supply 1990-2021\n",
    "supply_df = primary_source_df2.filter(primary_source_df2.EnergySource == \"Total\")\n",
    "\n",
    "supply_df = supply_df.withColumnRenamed(\"GenerationMil\", \"Supply\")\n",
    "\n",
    "supply_df2 = supply_df.select(\"YEAR\", \"Supply\")\n",
    "\n",
    "#Solar generation predictions from weather data, 5 year period + 2006 actual\n",
    "\n",
    "solar_preds_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/Solar_Predictions2010-2014.csv\")\n",
    "\n",
    "#Average value from preds and actual per farm size\n",
    "\n",
    "annual_100MW_df = solar_preds_df.select(avg(\"100MW\"))\n",
    "\n",
    "annual_100MW = annual_100MW_df.first()[\"avg(100MW)\"]\n",
    "\n",
    "annual_150MW_df = solar_preds_df.select(avg(\"150MW\"))\n",
    "\n",
    "annual_150MW = annual_150MW_df.first()[\"avg(150MW)\"]\n",
    "\n",
    "annual_200MW_df = solar_preds_df.select(avg(\"200MW\"))\n",
    "\n",
    "annual_200MW = annual_200MW_df.first()[\"avg(200MW)\"]\n",
    "\n",
    "# Energy loss -- \n",
    "# 9.5% of electricity lost from preinverter derate (DC losses) \n",
    "# 2.0% of energy lost from inverter efficiency (AC losses)\n",
    "\n",
    "annual_100MW = annual_100MW * .115\n",
    "\n",
    "annual_150MW = annual_150MW * .115\n",
    "\n",
    "annual_200MW = annual_200MW * .115\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e0db591",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "def getSupplyDemand(resource_list, reduction_list):\n",
    "        \n",
    "    #2021 primary source levels\n",
    "    source21_df = primary_source_df2.filter(primary_source_df2.YEAR == 2021)\n",
    "            \n",
    "    source21_total = source21_df.filter(source21_df.EnergySource == \"Total\")\n",
    "\n",
    "    generation21 = source21_total.first()[\"GenerationMil\"]\n",
    "    \n",
    "    coal21 = 2.752473\n",
    "    \n",
    "    petroleum21 = 0.008014\n",
    "    \n",
    "    naturalGas21 = 26.129918\n",
    "    \n",
    "    pred_data = []\n",
    "    \n",
    "    previous_supply = 0\n",
    "    \n",
    "    for year in range(2022, 2050):\n",
    "        if year == 2022:\n",
    "            pred_data.append((year, generation21))\n",
    "            previous_supply = generation21\n",
    "        else:\n",
    "            new_resources = 0\n",
    "            \n",
    "            #Add selected PV farms\n",
    "            for resource_tup in resource_list:\n",
    "                if resource_tup[1] == year:\n",
    "                    if resource_tup[0] == '100 MW':\n",
    "                        new_resources = new_resources + annual_100MW\n",
    "                    elif resource_tup[0] == '150 MW':\n",
    "                        new_resources = new_resources + annual_150MW\n",
    "                    elif resource_tup[0] == '200 MW':\n",
    "                        new_resources = new_resources + annual_200MW\n",
    "             \n",
    "            #Add previous year supply, convert new resources to millions scale \n",
    "            supply = previous_supply + (new_resources / 1000000)\n",
    "            \n",
    "            amount_to_remove = 0\n",
    "            \n",
    "            #Removed selected 2021 resources            \n",
    "            for removal_tup in resource_list:\n",
    "                if removal_tup[1] == year:\n",
    "                    if removal_tup[0] == 'Coal':\n",
    "                        amount_to_remove = amount_to_remove - coal21\n",
    "                    elif removal_tup[0] == 'Petroleum':\n",
    "                        amount_to_remove = amount_to_remove - petroleum21\n",
    "                    elif removal_tup[0] == 'Natural Gas 10%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .1)\n",
    "                    elif removal_tup[0] == 'Natural Gas 25%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .25)\n",
    "                    elif removal_tup[0] == 'Natural Gas 50%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .5)\n",
    "                    elif removal_tup[0] == 'Natural Gas 10%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .1)\n",
    "                    elif removal_tup[0] == 'Natural Gas 75%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .75)\n",
    "                    elif removal_tup[0] == 'Natural Gas 100%':\n",
    "                        amount_to_remove = amount_to_remove - naturalGas21\n",
    "            \n",
    "            #Remove selection from supply\n",
    "            supply = supply - amount_to_remove\n",
    "            \n",
    "            pred_data.append((year, supply))\n",
    "            \n",
    "            previous_supply = supply\n",
    "    \n",
    "    #Create dataframe for supply predictions\n",
    "    columns = ['YEAR', 'Supply']\n",
    "    \n",
    "    supply_pred_df = spark.createDataFrame(pred_data, columns)\n",
    "    \n",
    "    #Join with historical\n",
    "    supply_df_all = supply_df2.union(supply_pred_df)\n",
    "       \n",
    "    #Join with demand\n",
    "    supply_demand_df = supply_df_all.join(demand_df2, supply_df_all.YEAR == demand_df2.YEAR).drop(demand_df2.YEAR)\n",
    "    \n",
    "    supply_demand_panda = supply_demand_df.toPandas()\n",
    "    \n",
    "    return supply_demand_panda  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e82cb73",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 100MW PV panel farm\n",
    "\n",
    "solar100MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_100mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar100MW_df2 = solar100MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "734cc1d4",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 150MW PV panel farm\n",
    "\n",
    "solar150MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_150mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar150MW_df2 = solar150MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power150\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1fdd38e",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 200MW PV panel farm\n",
    "\n",
    "solar200MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_200mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar200MW_df2 = solar200MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c442b87a",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Group solar readings by month\n",
    "\n",
    "df_100 = solar100MW_df2.groupBy(\"Month\").sum(\"Power100\")\n",
    "\n",
    "df_100 = df_100.withColumnRenamed(\"sum(Power100)\", \"Sum100\")\n",
    "\n",
    "df_150 = solar150MW_df2.groupBy(\"Month\").sum(\"Power150\")\n",
    "\n",
    "df_150 = df_150.withColumnRenamed(\"sum(Power150)\", \"Sum150\")\n",
    "\n",
    "df_200 = solar200MW_df2.groupBy(\"Month\").sum(\"Power200\")\n",
    "\n",
    "df_200 = df_200.withColumnRenamed(\"sum(Power200)\", \"Sum200\")\n",
    "\n",
    "# Merge solar reading sets for visualization, order by month\n",
    "\n",
    "solar_readings_df = df_100.join(df_150, df_100.Month == df_150.Month).drop(df_150.Month)\n",
    "\n",
    "solar_readings_df2 = solar_readings_df.join(df_200, solar_readings_df.Month == df_200.Month).drop(df_200.Month)\n",
    "\n",
    "solar_readings_df3 = solar_readings_df2.orderBy(\"Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f080ddc",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert reading to monthly MWh\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMWh(z), DoubleType())\n",
    "\n",
    "def convertMWh(reading):\n",
    "    MWh = reading/730\n",
    "    return MWh\n",
    "\n",
    "solar_readings_df4 = solar_readings_df3.select(\"Month\", convertUDF(col(\"Sum100\").alias(\"100MW\")), \n",
    "                                               convertUDF(col(\"Sum150\").alias(\"1500MW\")), \n",
    "                                               convertUDF(col(\"Sum200\").alias(\"200MW\")))\n",
    "\n",
    "#Rename custom columns\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum100 AS 100MW)\", \"100 MW\")\n",
    "\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum150 AS 1500MW)\", \"150 MW\")\n",
    "\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum200 AS 200MW)\", \"200 MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53c3dcae",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Method to return solar generation panda frame to dashboard\n",
    "\n",
    "def createSGFrame():\n",
    "    \n",
    "    solar_readings_df5 = solar_readings_df4.toPandas()\n",
    "    \n",
    "    return solar_readings_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c375618d",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import data for solstice & equinox visualization\n",
    "\n",
    "se_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/SolsticeAndEquinoxCategorical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfe4a512",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Return data to dashboard for seasonal GHI visualization\n",
    "\n",
    "def createGHIFrame():\n",
    "    \n",
    "    se_df2 = se_df.toPandas()\n",
    "    \n",
    "    se_df_grouped = se_df2.groupby(\"Event\")\n",
    "    \n",
    "    return se_df_grouped"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
