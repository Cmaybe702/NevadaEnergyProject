{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4178501c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/courtney/opt/anaconda3/lib/python3.9/site-packages/pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "153e3852",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#sc = SparkContext.getOrCreate()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#sc.setLogLevel('OFF')\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNV Energy Project\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:274\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODULE$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplyModifiableSettings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1313\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1313\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1277\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1277\u001b[0m         (new_args, temp_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m         new_args \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1264\u001b[0m, in \u001b[0;36mJavaMember._get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m converter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39mconverters:\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mcan_convert(arg):\n\u001b[0;32m-> 1264\u001b[0m         temp_arg \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m         temp_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n\u001b[1;32m   1266\u001b[0m         new_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_collections.py:523\u001b[0m, in \u001b[0;36mMapConverter.convert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    521\u001b[0m java_map \u001b[38;5;241m=\u001b[39m HashMap()\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 523\u001b[0m     java_map[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m[key]\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_map\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_collections.py:82\u001b[0m, in \u001b[0;36mJavaMap.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1313\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1313\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1283\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m   1280\u001b[0m     temp_args \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1282\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1283\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args_command, temp_args\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1283\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m   1280\u001b[0m     temp_args \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1282\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1283\u001b[0m     [\u001b[43mget_command_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args_command, temp_args\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py:298\u001b[0m, in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    296\u001b[0m         command_part \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m interface\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     command_part \u001b[38;5;241m=\u001b[39m REFERENCE_TYPE \u001b[38;5;241m+\u001b[39m \u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_id\u001b[49m()\n\u001b[1;32m    300\u001b[0m command_part \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m command_part\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.functions import translate, col, month\n",
    "from pyspark.sql.types import DoubleType, TimestampType, StringType\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NV Energy Project\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "sc.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe7f4bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.driver.port', '52297'),\n",
       " ('spark.driver.host', '192.168.0.6'),\n",
       " ('spark.sql.warehouse.dir', 'file:/Users/courtney/voila/spark-warehouse'),\n",
       " ('spark.app.submitTime', '1679509682073'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1679509682235'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1679509683198'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b54e12c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "## ENERGY CONSUMPTION MODEL VS ACTUAL CONSUMPTION\n",
    "#Import consumption data\n",
    "\n",
    "nv_ac_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/nv_annual_consumption60-21.csv\")\n",
    "\n",
    "nv_ac_df = nv_ac_df.withColumn(\"TotalConsumption\", translate(col(\"TotalConsumption\"), \",\", \"\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18b6040",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert data to millions scale for visualization\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "nv_ac_df2 = nv_ac_df.select(\"YEAR\", \"TotalConsumption\", convertUDF(col(\"TotalConsumption\").alias(\"TotalMillions\"))) \n",
    "\n",
    "nv_ac_df2 = nv_ac_df2.withColumnRenamed(\"<lambda>(TotalConsumption AS TotalMillions)\", \"Actual Consumption\")\n",
    "\n",
    "nv_ac_df3 = nv_ac_df2.filter(nv_ac_df2.YEAR != 1990)\n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.filter(nv_ac_df2.YEAR != 2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17c582b",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Get predictions from Consumption LR \n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "convertUDF = udf(lambda z: predict(z), DoubleType())\n",
    "\n",
    "def predict(x):\n",
    "    return  x * 675250.9602239017 - 1325137830.201505\n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.select(\"YEAR\", \"TotalConsumption\", \"Actual Consumption\", \n",
    "                             convertUDF(col(\"YEAR\").alias(\"ModelPred\"))) \n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.withColumnRenamed(\"<lambda>(YEAR AS ModelPred)\", \"ModelPred\")\n",
    "\n",
    "#Convert predictions to millions scale\n",
    "\n",
    "convertUDF2 = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "nv_ac_df4 = nv_ac_df3.select(\"YEAR\", \"TotalConsumption\", \"Actual Consumption\", \"ModelPred\",\n",
    "                             convertUDF2(col(\"ModelPred\").alias(\"PredMillions\"))) \n",
    "\n",
    "nv_ac_df4 = nv_ac_df4.withColumnRenamed(\"<lambda>(ModelPred AS PredMillions)\", \"Model Predictions\")\n",
    "\n",
    "nv_ac_df4 = nv_ac_df4.orderBy(\"YEAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb608d7",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create consumption panda frame, return to dashboard\n",
    "\n",
    "def createNVCFrame():\n",
    "    \n",
    "    nv_ac_df5 = nv_ac_df4.toPandas()\n",
    "    \n",
    "    return nv_ac_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00465b47",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "## NEVADA ENERGY PRODUCTION BY PRIMARY SOURCE\n",
    "#Import primary source data\n",
    "\n",
    "primary_source_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/GenerationBySource.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf460753",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert data to millions scale for visualization\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    \n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "#primary_source_df2 = primary_source_df.filter(primary_source_df.energySource != \"Total\")\n",
    "\n",
    "primary_source_df2 = primary_source_df.select(\"YEAR\", \"EnergySource\", convertUDF(col(\"GenerationMWh\").alias(\"TotalMillions\"))) \n",
    "\n",
    "#Order by year and rename custom column\n",
    "primary_source_df2 = primary_source_df2.orderBy(\"YEAR\")\n",
    "\n",
    "primary_source_df2 = primary_source_df2.withColumnRenamed(\"<lambda>(GenerationMWh AS TotalMillions)\", \"GenerationMil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7914411c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create primary source pandas frame for dashboard\n",
    "\n",
    "def createPSFrame():\n",
    "    ps_df = primary_source_df2.toPandas()\n",
    "\n",
    "    ps_df_grouped = ps_df.groupby(\"EnergySource\")\n",
    "\n",
    "    return ps_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8849729",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "##SUPPLY AND DEMAND\n",
    "#Dataframe for interactive demand\n",
    "\n",
    "hist_demand_df = nv_ac_df.filter(nv_ac_df.YEAR >= 1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90cd83a4",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create demand prediction frame\n",
    "\n",
    "pred_df = spark.range(2022,2050).withColumnRenamed(\"id\",\"YEAR\")\n",
    "\n",
    "convertUDF = udf(lambda z: predict(z), DoubleType())\n",
    "\n",
    "def predict(x):\n",
    "    return  x * 675250.9602239017 - 1325137830.201505\n",
    "\n",
    "pred_df2 = pred_df.select(\"YEAR\", convertUDF(col(\"YEAR\").alias(\"ModelPred\"))) \n",
    "\n",
    "pred_df2 = pred_df2.withColumnRenamed(\"<lambda>(YEAR AS ModelPred\", \"Demand\")\n",
    "\n",
    "#Merge actual and predictions\n",
    "\n",
    "demand_df = hist_demand_df.union(pred_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4cecbc",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert demand to millions scale\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    \n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "demand_df2 = demand_df.select(\"YEAR\", convertUDF(col(\"TotalConsumption\").alias(\"Demand\")))\n",
    "\n",
    "demand_df2 = demand_df2.withColumnRenamed(\"<lambda>(TotalConsumption AS Demand)\", \"Demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aca818f",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "#Dataframe for interactive supply\n",
    "\n",
    "#Historical supply 1990-2021\n",
    "supply_df = primary_source_df2.filter(primary_source_df2.EnergySource == \"Total\")\n",
    "\n",
    "supply_df = supply_df.withColumnRenamed(\"GenerationMil\", \"Supply\")\n",
    "\n",
    "supply_df2 = supply_df.select(\"YEAR\", \"Supply\")\n",
    "\n",
    "#Solar generation predictions from weather data, 5 year period + 2006 actual\n",
    "\n",
    "solar_preds_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/Solar_Predictions2010-2014.csv\")\n",
    "\n",
    "#Average value from preds and actual per farm size\n",
    "\n",
    "annual_100MW_df = solar_preds_df.select(avg(\"100MW\"))\n",
    "\n",
    "annual_100MW = annual_100MW_df.first()[\"avg(100MW)\"]\n",
    "\n",
    "annual_150MW_df = solar_preds_df.select(avg(\"150MW\"))\n",
    "\n",
    "annual_150MW = annual_150MW_df.first()[\"avg(150MW)\"]\n",
    "\n",
    "annual_200MW_df = solar_preds_df.select(avg(\"200MW\"))\n",
    "\n",
    "annual_200MW = annual_200MW_df.first()[\"avg(200MW)\"]\n",
    "\n",
    "# Energy loss -- \n",
    "# 9.5% of electricity lost from preinverter derate (DC losses) \n",
    "# 2.0% of energy lost from inverter efficiency (AC losses)\n",
    "\n",
    "annual_100MW = annual_100MW * .115\n",
    "\n",
    "annual_150MW = annual_150MW * .115\n",
    "\n",
    "annual_200MW = annual_200MW * .115\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e0db591",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "def getSupplyDemand(resource_list, reduction_list):\n",
    "        \n",
    "    #2021 primary source levels\n",
    "    source21_df = primary_source_df2.filter(primary_source_df2.YEAR == 2021)\n",
    "            \n",
    "    source21_total = source21_df.filter(source21_df.EnergySource == \"Total\")\n",
    "\n",
    "    generation21 = source21_total.first()[\"GenerationMil\"]\n",
    "    \n",
    "    coal21 = 2.752473\n",
    "    \n",
    "    petroleum21 = 0.008014\n",
    "    \n",
    "    naturalGas21 = 26.129918\n",
    "    \n",
    "    pred_data = []\n",
    "    \n",
    "    previous_supply = 0\n",
    "    \n",
    "    for year in range(2022, 2050):\n",
    "        if year == 2022:\n",
    "            pred_data.append((year, generation21))\n",
    "            previous_supply = generation21\n",
    "        else:\n",
    "            new_resources = 0\n",
    "            \n",
    "            #Add selected PV farms\n",
    "            for resource_tup in resource_list:\n",
    "                if resource_tup[1] == year:\n",
    "                    if resource_tup[0] == '100 MW':\n",
    "                        new_resources = new_resources + annual_100MW\n",
    "                    elif resource_tup[0] == '150 MW':\n",
    "                        new_resources = new_resources + annual_150MW\n",
    "                    elif resource_tup[0] == '200 MW':\n",
    "                        new_resources = new_resources + annual_200MW\n",
    "             \n",
    "            #Add previous year supply, convert new resources to millions scale \n",
    "            supply = previous_supply + (new_resources / 1000000)\n",
    "            \n",
    "            amount_to_remove = 0\n",
    "            \n",
    "            #Removed selected 2021 resources            \n",
    "            for removal_tup in resource_list:\n",
    "                if removal_tup[1] == year:\n",
    "                    if removal_tup[0] == 'Coal':\n",
    "                        amount_to_remove = amount_to_remove - coal21\n",
    "                    elif removal_tup[0] == 'Petroleum':\n",
    "                        amount_to_remove = amount_to_remove - petroleum21\n",
    "                    elif removal_tup[0] == 'Natural Gas 10%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .1)\n",
    "                    elif removal_tup[0] == 'Natural Gas 25%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .25)\n",
    "                    elif removal_tup[0] == 'Natural Gas 50%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .5)\n",
    "                    elif removal_tup[0] == 'Natural Gas 10%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .1)\n",
    "                    elif removal_tup[0] == 'Natural Gas 75%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .75)\n",
    "                    elif removal_tup[0] == 'Natural Gas 100%':\n",
    "                        amount_to_remove = amount_to_remove - naturalGas21\n",
    "            \n",
    "            #Remove selection from supply\n",
    "            supply = supply - amount_to_remove\n",
    "            \n",
    "            pred_data.append((year, supply))\n",
    "            \n",
    "            previous_supply = supply\n",
    "    \n",
    "    #Create dataframe for supply predictions\n",
    "    columns = ['YEAR', 'Supply']\n",
    "    \n",
    "    supply_pred_df = spark.createDataFrame(pred_data, columns)\n",
    "    \n",
    "    #Join with historical\n",
    "    supply_df_all = supply_df2.union(supply_pred_df)\n",
    "       \n",
    "    #Join with demand\n",
    "    supply_demand_df = supply_df_all.join(demand_df2, supply_df_all.YEAR == demand_df2.YEAR).drop(demand_df2.YEAR)\n",
    "    \n",
    "    supply_demand_panda = supply_demand_df.toPandas()\n",
    "    \n",
    "    return supply_demand_panda  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e82cb73",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 100MW PV panel farm\n",
    "\n",
    "solar100MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_100mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar100MW_df2 = solar100MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "734cc1d4",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 150MW PV panel farm\n",
    "\n",
    "solar150MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_150mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar150MW_df2 = solar150MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power150\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1fdd38e",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 200MW PV panel farm\n",
    "\n",
    "solar200MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_200mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar200MW_df2 = solar200MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c442b87a",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Group solar readings by month\n",
    "\n",
    "df_100 = solar100MW_df2.groupBy(\"Month\").sum(\"Power100\")\n",
    "\n",
    "df_100 = df_100.withColumnRenamed(\"sum(Power100)\", \"Sum100\")\n",
    "\n",
    "df_150 = solar150MW_df2.groupBy(\"Month\").sum(\"Power150\")\n",
    "\n",
    "df_150 = df_150.withColumnRenamed(\"sum(Power150)\", \"Sum150\")\n",
    "\n",
    "df_200 = solar200MW_df2.groupBy(\"Month\").sum(\"Power200\")\n",
    "\n",
    "df_200 = df_200.withColumnRenamed(\"sum(Power200)\", \"Sum200\")\n",
    "\n",
    "# Merge solar reading sets for visualization, order by month\n",
    "\n",
    "solar_readings_df = df_100.join(df_150, df_100.Month == df_150.Month).drop(df_150.Month)\n",
    "\n",
    "solar_readings_df2 = solar_readings_df.join(df_200, solar_readings_df.Month == df_200.Month).drop(df_200.Month)\n",
    "\n",
    "solar_readings_df3 = solar_readings_df2.orderBy(\"Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f080ddc",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert reading to monthly MWh\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMWh(z), DoubleType())\n",
    "\n",
    "def convertMWh(reading):\n",
    "    MWh = reading/730\n",
    "    return MWh\n",
    "\n",
    "solar_readings_df4 = solar_readings_df3.select(\"Month\", convertUDF(col(\"Sum100\").alias(\"100MW\")), \n",
    "                                               convertUDF(col(\"Sum150\").alias(\"1500MW\")), \n",
    "                                               convertUDF(col(\"Sum200\").alias(\"200MW\")))\n",
    "\n",
    "#Rename custom columns\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum100 AS 100MW)\", \"100 MW\")\n",
    "\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum150 AS 1500MW)\", \"150 MW\")\n",
    "\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum200 AS 200MW)\", \"200 MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53c3dcae",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Method to return solar generation panda frame to dashboard\n",
    "\n",
    "def createSGFrame():\n",
    "    \n",
    "    solar_readings_df5 = solar_readings_df4.toPandas()\n",
    "    \n",
    "    return solar_readings_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c375618d",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import data for solstice & equinox visualization\n",
    "\n",
    "se_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/SolsticeAndEquinoxCategorical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfe4a512",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Return data to dashboard for seasonal GHI visualization\n",
    "\n",
    "def createGHIFrame():\n",
    "    \n",
    "    se_df2 = se_df.toPandas()\n",
    "    \n",
    "    se_df_grouped = se_df2.groupby(\"Event\")\n",
    "    \n",
    "    return se_df_grouped"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
