{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4178501c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/opt/apache-spark/libexec'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153e3852",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/30 11:41:31 WARN Utils: Your hostname, Courtneys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.6 instead (on interface en0)\n",
      "23/03/30 11:41:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/30 11:41:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.functions import translate, col, month\n",
    "from pyspark.sql.types import DoubleType, TimestampType, StringType\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NV Energy Project\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "sc.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b54e12c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "## ENERGY CONSUMPTION MODEL VS ACTUAL CONSUMPTION\n",
    "#Import consumption data\n",
    "\n",
    "nv_ac_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/nv_annual_consumption60-21.csv\")\n",
    "\n",
    "nv_ac_df = nv_ac_df.withColumn(\"TotalConsumption\", translate(col(\"TotalConsumption\"), \",\", \"\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18b6040",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert data to millions scale for visualization\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "nv_ac_df2 = nv_ac_df.select(\"YEAR\", \"TotalConsumption\", convertUDF(col(\"TotalConsumption\").alias(\"TotalMillions\"))) \n",
    "\n",
    "nv_ac_df2 = nv_ac_df2.withColumnRenamed(\"<lambda>(TotalConsumption AS TotalMillions)\", \"Actual Consumption\")\n",
    "\n",
    "nv_ac_df3 = nv_ac_df2.filter(nv_ac_df2.YEAR != 1990)\n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.filter(nv_ac_df2.YEAR != 2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17c582b",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Get predictions from Consumption LR \n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "convertUDF = udf(lambda z: predict(z), DoubleType())\n",
    "\n",
    "def predict(x):\n",
    "    return  x * 675250.9602239017 - 1325137830.201505\n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.select(\"YEAR\", \"TotalConsumption\", \"Actual Consumption\", \n",
    "                             convertUDF(col(\"YEAR\").alias(\"ModelPred\"))) \n",
    "\n",
    "nv_ac_df3 = nv_ac_df3.withColumnRenamed(\"<lambda>(YEAR AS ModelPred)\", \"ModelPred\")\n",
    "\n",
    "#Convert predictions to millions scale\n",
    "\n",
    "convertUDF2 = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "nv_ac_df4 = nv_ac_df3.select(\"YEAR\", \"TotalConsumption\", \"Actual Consumption\", \"ModelPred\",\n",
    "                             convertUDF2(col(\"ModelPred\").alias(\"PredMillions\"))) \n",
    "\n",
    "nv_ac_df4 = nv_ac_df4.withColumnRenamed(\"<lambda>(ModelPred AS PredMillions)\", \"Model Predictions\")\n",
    "\n",
    "nv_ac_df4 = nv_ac_df4.orderBy(\"YEAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb608d7",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create consumption panda frame, return to dashboard\n",
    "\n",
    "def createNVCFrame():\n",
    "    \n",
    "    nv_ac_df5 = nv_ac_df4.toPandas()\n",
    "    \n",
    "    return nv_ac_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00465b47",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "## NEVADA ENERGY PRODUCTION BY PRIMARY SOURCE\n",
    "#Import primary source data\n",
    "\n",
    "primary_source_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/GenerationBySource.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf460753",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert data to millions scale for visualization\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    \n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "#primary_source_df2 = primary_source_df.filter(primary_source_df.energySource != \"Total\")\n",
    "\n",
    "primary_source_df2 = primary_source_df.select(\"YEAR\", \"EnergySource\", convertUDF(col(\"GenerationMWh\").alias(\"TotalMillions\"))) \n",
    "\n",
    "#Order by year and rename custom column\n",
    "primary_source_df2 = primary_source_df2.orderBy(\"YEAR\")\n",
    "\n",
    "primary_source_df2 = primary_source_df2.withColumnRenamed(\"<lambda>(GenerationMWh AS TotalMillions)\", \"GenerationMil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7914411c",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create primary source pandas frame for dashboard\n",
    "\n",
    "def createPSFrame():\n",
    "    ps_df = primary_source_df2.toPandas()\n",
    "\n",
    "    ps_df_grouped = ps_df.groupby(\"EnergySource\")\n",
    "\n",
    "    return ps_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8849729",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "##SUPPLY AND DEMAND\n",
    "#Dataframe for interactive demand\n",
    "\n",
    "hist_demand_df = nv_ac_df.filter(nv_ac_df.YEAR >= 1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90cd83a4",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Create demand prediction frame\n",
    "\n",
    "pred_df = spark.range(2022,2050).withColumnRenamed(\"id\",\"YEAR\")\n",
    "\n",
    "convertUDF = udf(lambda z: predict(z), DoubleType())\n",
    "\n",
    "def predict(x):\n",
    "    return  x * 675250.9602239017 - 1325137830.201505\n",
    "\n",
    "pred_df2 = pred_df.select(\"YEAR\", convertUDF(col(\"YEAR\").alias(\"ModelPred\"))) \n",
    "\n",
    "pred_df2 = pred_df2.withColumnRenamed(\"<lambda>(YEAR AS ModelPred\", \"Demand\")\n",
    "\n",
    "#Merge actual and predictions\n",
    "\n",
    "demand_df = hist_demand_df.union(pred_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4cecbc",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert demand to millions scale\n",
    "\n",
    "convertUDF = udf(lambda z: convertMill(z), DoubleType())\n",
    "\n",
    "def convertMill(reading):\n",
    "    \n",
    "    new = reading/1000000\n",
    "    return new\n",
    "\n",
    "demand_df2 = demand_df.select(\"YEAR\", convertUDF(col(\"TotalConsumption\").alias(\"Demand\")))\n",
    "\n",
    "demand_df2 = demand_df2.withColumnRenamed(\"<lambda>(TotalConsumption AS Demand)\", \"Demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aca818f",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "#Dataframe for interactive supply\n",
    "\n",
    "#Historical supply 1990-2021\n",
    "supply_df = primary_source_df2.filter(primary_source_df2.EnergySource == \"Total\")\n",
    "\n",
    "supply_df = supply_df.withColumnRenamed(\"GenerationMil\", \"Supply\")\n",
    "\n",
    "supply_df2 = supply_df.select(\"YEAR\", \"Supply\")\n",
    "\n",
    "#Solar generation predictions from weather data, 5 year period + 2006 actual\n",
    "\n",
    "solar_preds_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/Solar_Predictions2010-2014.csv\")\n",
    "\n",
    "#Average value from preds and actual per farm size\n",
    "\n",
    "annual_100MW_df = solar_preds_df.select(avg(\"100MW\"))\n",
    "\n",
    "annual_100MW = annual_100MW_df.first()[\"avg(100MW)\"]\n",
    "\n",
    "annual_150MW_df = solar_preds_df.select(avg(\"150MW\"))\n",
    "\n",
    "annual_150MW = annual_150MW_df.first()[\"avg(150MW)\"]\n",
    "\n",
    "annual_200MW_df = solar_preds_df.select(avg(\"200MW\"))\n",
    "\n",
    "annual_200MW = annual_200MW_df.first()[\"avg(200MW)\"]\n",
    "\n",
    "# Energy loss -- \n",
    "# 9.5% of electricity lost from preinverter derate (DC losses) \n",
    "# 2.0% of energy lost from inverter efficiency (AC losses)\n",
    "\n",
    "annual_100MW = annual_100MW * .115\n",
    "\n",
    "annual_150MW = annual_150MW * .115\n",
    "\n",
    "annual_200MW = annual_200MW * .115\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e0db591",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "def getSupplyDemand(resource_list, reduction_list):\n",
    "        \n",
    "    #2021 primary source levels\n",
    "    source21_df = primary_source_df2.filter(primary_source_df2.YEAR == 2021)\n",
    "            \n",
    "    source21_total = source21_df.filter(source21_df.EnergySource == \"Total\")\n",
    "\n",
    "    generation21 = source21_total.first()[\"GenerationMil\"]\n",
    "    \n",
    "    coal21 = 2.752473\n",
    "    \n",
    "    petroleum21 = 0.008014\n",
    "    \n",
    "    naturalGas21 = 26.129918\n",
    "    \n",
    "    pred_data = []\n",
    "    \n",
    "    previous_supply = 0\n",
    "    \n",
    "    for year in range(2022, 2050):\n",
    "        if year == 2022:\n",
    "            pred_data.append((year, generation21))\n",
    "            previous_supply = generation21\n",
    "        else:\n",
    "            new_resources = 0\n",
    "            \n",
    "            #Add selected PV farms\n",
    "            for resource_tup in resource_list:\n",
    "                if resource_tup[1] == year:\n",
    "                    if resource_tup[0] == '100 MW':\n",
    "                        new_resources = new_resources + annual_100MW\n",
    "                    elif resource_tup[0] == '150 MW':\n",
    "                        new_resources = new_resources + annual_150MW\n",
    "                    elif resource_tup[0] == '200 MW':\n",
    "                        new_resources = new_resources + annual_200MW\n",
    "             \n",
    "            #Add previous year supply, convert new resources to millions scale \n",
    "            supply = previous_supply + (new_resources / 1000000)\n",
    "            \n",
    "            amount_to_remove = 0\n",
    "            \n",
    "            #Removed selected 2021 resources            \n",
    "            for removal_tup in resource_list:\n",
    "                if removal_tup[1] == year:\n",
    "                    if removal_tup[0] == 'Coal':\n",
    "                        amount_to_remove = amount_to_remove - coal21\n",
    "                    elif removal_tup[0] == 'Petroleum':\n",
    "                        amount_to_remove = amount_to_remove - petroleum21\n",
    "                    elif removal_tup[0] == 'Natural Gas 10%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .1)\n",
    "                    elif removal_tup[0] == 'Natural Gas 25%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .25)\n",
    "                    elif removal_tup[0] == 'Natural Gas 50%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .5)\n",
    "                    elif removal_tup[0] == 'Natural Gas 10%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .1)\n",
    "                    elif removal_tup[0] == 'Natural Gas 75%':\n",
    "                        amount_to_remove = amount_to_remove - (naturalGas21 * .75)\n",
    "                    elif removal_tup[0] == 'Natural Gas 100%':\n",
    "                        amount_to_remove = amount_to_remove - naturalGas21\n",
    "            \n",
    "            #Remove selection from supply\n",
    "            supply = supply - amount_to_remove\n",
    "            \n",
    "            pred_data.append((year, supply))\n",
    "            \n",
    "            previous_supply = supply\n",
    "    \n",
    "    #Create dataframe for supply predictions\n",
    "    columns = ['YEAR', 'Supply']\n",
    "    \n",
    "    supply_pred_df = spark.createDataFrame(pred_data, columns)\n",
    "    \n",
    "    #Join with historical\n",
    "    supply_df_all = supply_df2.union(supply_pred_df)\n",
    "       \n",
    "    #Join with demand\n",
    "    supply_demand_df = supply_df_all.join(demand_df2, supply_df_all.YEAR == demand_df2.YEAR).drop(demand_df2.YEAR)\n",
    "    \n",
    "    supply_demand_panda = supply_demand_df.toPandas()\n",
    "    \n",
    "    return supply_demand_panda  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e82cb73",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 100MW PV panel farm\n",
    "\n",
    "solar100MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_100mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar100MW_df2 = solar100MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "734cc1d4",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 150MW PV panel farm\n",
    "\n",
    "solar150MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_150mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar150MW_df2 = solar150MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power150\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1fdd38e",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import actual readings from 200MW PV panel farm\n",
    "\n",
    "solar200MW_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/solar_actual_200mw.csv\")\n",
    "\n",
    "#Add month for visualizations\n",
    "solar200MW_df2 = solar200MW_df.select(\"Timestamp\", month(\"Timestamp\").alias(\"Month\"), \"Power200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c442b87a",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Group solar readings by month\n",
    "\n",
    "df_100 = solar100MW_df2.groupBy(\"Month\").sum(\"Power100\")\n",
    "\n",
    "df_100 = df_100.withColumnRenamed(\"sum(Power100)\", \"Sum100\")\n",
    "\n",
    "df_150 = solar150MW_df2.groupBy(\"Month\").sum(\"Power150\")\n",
    "\n",
    "df_150 = df_150.withColumnRenamed(\"sum(Power150)\", \"Sum150\")\n",
    "\n",
    "df_200 = solar200MW_df2.groupBy(\"Month\").sum(\"Power200\")\n",
    "\n",
    "df_200 = df_200.withColumnRenamed(\"sum(Power200)\", \"Sum200\")\n",
    "\n",
    "# Merge solar reading sets for visualization, order by month\n",
    "\n",
    "solar_readings_df = df_100.join(df_150, df_100.Month == df_150.Month).drop(df_150.Month)\n",
    "\n",
    "solar_readings_df2 = solar_readings_df.join(df_200, solar_readings_df.Month == df_200.Month).drop(df_200.Month)\n",
    "\n",
    "solar_readings_df3 = solar_readings_df2.orderBy(\"Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f080ddc",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Convert reading to monthly MWh\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "convertUDF = udf(lambda z: convertMWh(z), DoubleType())\n",
    "\n",
    "def convertMWh(reading):\n",
    "    MWh = reading/730\n",
    "    return MWh\n",
    "\n",
    "solar_readings_df4 = solar_readings_df3.select(\"Month\", convertUDF(col(\"Sum100\").alias(\"100MW\")), \n",
    "                                               convertUDF(col(\"Sum150\").alias(\"1500MW\")), \n",
    "                                               convertUDF(col(\"Sum200\").alias(\"200MW\")))\n",
    "\n",
    "#Rename custom columns\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum100 AS 100MW)\", \"100 MW\")\n",
    "\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum150 AS 1500MW)\", \"150 MW\")\n",
    "\n",
    "solar_readings_df4 = solar_readings_df4.withColumnRenamed(\"<lambda>(Sum200 AS 200MW)\", \"200 MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53c3dcae",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Method to return solar generation panda frame to dashboard\n",
    "\n",
    "def createSGFrame():\n",
    "    \n",
    "    solar_readings_df5 = solar_readings_df4.toPandas()\n",
    "    \n",
    "    return solar_readings_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c375618d",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Import data for solstice & equinox visualization\n",
    "\n",
    "se_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"CleanData/SolsticeAndEquinoxCategorical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfe4a512",
   "metadata": {
    "tags": [
     "nbconvert-hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Return data to dashboard for seasonal GHI visualization\n",
    "\n",
    "def createGHIFrame():\n",
    "    \n",
    "    se_df2 = se_df.toPandas()\n",
    "    \n",
    "    se_df_grouped = se_df2.groupby(\"Event\")\n",
    "    \n",
    "    return se_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b924608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.4.0\r\n",
      "anaconda-client==1.11.0\r\n",
      "anaconda-navigator==2.1.4\r\n",
      "anyio @ file:///opt/concourse/worker/volumes/live/485b0f52-1188-482a-6285-65a36c8fa8a6/volume/anyio_1644481714856/work/dist\r\n",
      "appnope @ file:///opt/concourse/worker/volumes/live/6ca6f098-d773-4461-5c91-a24a17435bda/volume/appnope_1606859448531/work\r\n",
      "argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work\r\n",
      "argon2-cffi-bindings @ file:///opt/concourse/worker/volumes/live/42cf1b28-e71f-45ed-47b2-50f828088636/volume/argon2-cffi-bindings_1644569709119/work\r\n",
      "asgiref==3.6.0\r\n",
      "asttokens @ file:///opt/conda/conda-bld/asttokens_1646925590279/work\r\n",
      "astunparse==1.6.3\r\n",
      "attrs @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_33k1uces4n/croot/attrs_1668696162258/work\r\n",
      "Babel @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_59c7q3smap/croot/babel_1671781946809/work\r\n",
      "backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\r\n",
      "backports.functools-lru-cache @ file:///tmp/build/80754af9/backports.functools_lru_cache_1618170165463/work\r\n",
      "backports.tempfile @ file:///home/linux1/recipes/ci/backports.tempfile_1610991236607/work\r\n",
      "backports.weakref==1.0.post1\r\n",
      "beautifulsoup4 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_croot-cdiouih5/beautifulsoup4_1650462164803/work\r\n",
      "bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work\r\n",
      "brotlipy==0.7.0\r\n",
      "cachetools==5.3.0\r\n",
      "certifi @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_83242e7e-f82d-4a71-8ef2-9d71d212d249gu_wxmeq/croots/recipe/certifi_1655968827803/work/certifi\r\n",
      "cffi @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_62rp5d8fd4/croots/recipe/cffi_1659598655556/work\r\n",
      "chardet @ file:///opt/concourse/worker/volumes/live/7e1102c4-8702-40f2-63d6-f260ce5f85e4/volume/chardet_1607706831384/work\r\n",
      "charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work\r\n",
      "click @ file:///opt/concourse/worker/volumes/live/17ca243b-fc66-462b-4bc1-f11ad524e336/volume/click_1646056621177/work\r\n",
      "clyent==1.2.2\r\n",
      "comm @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0b9r9i3b7k/croot/comm_1671231125581/work\r\n",
      "conda==23.1.0\r\n",
      "conda-build==3.23.3\r\n",
      "conda-content-trust @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0afea0f9-bbf5-446c-8d60-8151cc05b029p0nco1nn/croots/recipe/conda-content-trust_1658126375910/work\r\n",
      "conda-package-handling @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_f5r7ncch6s/croot/conda-package-handling_1672865026931/work\r\n",
      "conda-repo-cli==1.0.27\r\n",
      "conda-token @ file:///Users/paulyim/miniconda3/envs/c3i/conda-bld/conda-token_1662660369760/work\r\n",
      "conda-verify==3.4.2\r\n",
      "conda_package_streaming @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_619dxy_gif/croot/conda-package-streaming_1670508154637/work\r\n",
      "contourpy==1.0.7\r\n",
      "cryptography @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0ayq9hu973/croot/cryptography_1673298756837/work\r\n",
      "cycler==0.11.0\r\n",
      "debugpy @ file:///opt/concourse/worker/volumes/live/fb0af3c0-b0a4-42c5-46e7-6ce9c84da602/volume/debugpy_1637091827531/work\r\n",
      "decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work\r\n",
      "defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\r\n",
      "distlib==0.3.6\r\n",
      "Django==3.0.3\r\n",
      "entrypoints @ file:///opt/concourse/worker/volumes/live/78b2c4ae-da17-4aa7-7fdc-138f840abf07/volume/entrypoints_1649926486598/work\r\n",
      "executing @ file:///opt/conda/conda-bld/executing_1646925071911/work\r\n",
      "fastjsonschema @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b5c1gee32t/croots/recipe/python-fastjsonschema_1661368622875/work\r\n",
      "filelock @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_f29yrmlw9_/croot/filelock_1672387130651/work\r\n",
      "findspark @ file:///home/conda/feedstock_root/build_artifacts/findspark_1644599740637/work\r\n",
      "flatbuffers==23.1.21\r\n",
      "flit_core @ file:///opt/conda/conda-bld/flit-core_1644941570762/work/source/flit_core\r\n",
      "fonttools==4.38.0\r\n",
      "future @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_27i98bxita/croot/future_1677599886956/work\r\n",
      "gast==0.4.0\r\n",
      "glob2 @ file:///home/linux1/recipes/ci/glob2_1610991677669/work\r\n",
      "google-auth==2.16.1\r\n",
      "google-auth-oauthlib==0.4.6\r\n",
      "google-pasta==0.2.0\r\n",
      "grpcio==1.51.3\r\n",
      "h5py==3.8.0\r\n",
      "idna @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_00jf0h4zbt/croot/idna_1666125573348/work\r\n",
      "importlib-metadata @ file:///opt/concourse/worker/volumes/live/04344671-a59e-4800-6391-124f9c0e5484/volume/importlib-metadata_1648562417587/work\r\n",
      "importlib-resources==5.12.0\r\n",
      "ipykernel @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_4dybncc18w/croot/ipykernel_1671488388285/work\r\n",
      "ipympl==0.9.3\r\n",
      "ipython @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_5d1j6t_z43/croot/ipython_1676584167910/work\r\n",
      "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\r\n",
      "ipywidgets==8.0.2\r\n",
      "jedi @ file:///opt/concourse/worker/volumes/live/89be3eb0-a85c-453b-67dd-f706f2fa4c43/volume/jedi_1644315269953/work\r\n",
      "jieba==0.42.1\r\n",
      "Jinja2 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_6adj7x0ejx/croot/jinja2_1666908137966/work\r\n",
      "joblib @ file:///tmp/build/80754af9/joblib_1635411271373/work\r\n",
      "json5 @ file:///tmp/build/80754af9/json5_1624432770122/work\r\n",
      "jsonschema @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_21cqeq1xnk/croot/jsonschema_1676558686956/work\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-console==6.6.2\r\n",
      "jupyter-server @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_031akrjssy/croot/jupyter_server_1671707631142/work\r\n",
      "jupyter_client==7.4.1\r\n",
      "jupyter_core @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_cdq4mx8fk2/croot/jupyter_core_1676538594191/work\r\n",
      "jupyterlab @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_a52a346vyc/croot/jupyterlab_1675354129045/work\r\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\r\n",
      "jupyterlab-widgets==3.0.5\r\n",
      "jupyterlab_server @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_447yg3joq7/croot/jupyterlab_server_1677149708049/work\r\n",
      "kafka-python==2.0.2\r\n",
      "keras==2.11.0\r\n",
      "kiwisolver==1.4.4\r\n",
      "libarchive-c @ file:///tmp/build/80754af9/python-libarchive-c_1617780486945/work\r\n",
      "libclang==15.0.6.1\r\n",
      "lxml @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_1902c961-4bd2-4871-a3c5-70b7317a6521kpj7nz2o/croots/recipe/lxml_1657545138937/work\r\n",
      "Markdown==3.4.1\r\n",
      "MarkupSafe @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_d4a9444f-bd4c-4043-b47d-cede33979b0fve7bm42r/croots/recipe/markupsafe_1654597878200/work\r\n",
      "matplotlib==3.7.0\r\n",
      "matplotlib-inline @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9ddl71oqte/croots/recipe/matplotlib-inline_1662014471815/work\r\n",
      "mistune @ file:///opt/concourse/worker/volumes/live/4217afd5-dad1-438d-6f79-e4992ccda0e5/volume/mistune_1607364880245/work\r\n",
      "mkl-fft==1.3.1\r\n",
      "mkl-random @ file:///opt/concourse/worker/volumes/live/0cda23d8-7460-44b2-7e5d-3c76a8a0ca7e/volume/mkl_random_1626186083266/work\r\n",
      "mkl-service==2.4.0\r\n",
      "navigator-updater==0.2.1\r\n",
      "nbclassic @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_7c3czojxw1/croot/nbclassic_1676902906096/work\r\n",
      "nbclient @ file:///opt/concourse/worker/volumes/live/fcea0efc-2a08-48fd-5c55-85ef78e0ea28/volume/nbclient_1650308406463/work\r\n",
      "nbconvert @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_8fyzuglni_/croot/nbconvert_1668450649428/work\r\n",
      "nbformat @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2daun1fill/croot/nbformat_1670352339504/work\r\n",
      "nest-asyncio @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_64pfm74mxq/croot/nest-asyncio_1672387129786/work\r\n",
      "notebook @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0cdyriuhi_/croot/notebook_1668179888986/work\r\n",
      "notebook_shim @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_e9s6zsmlb7/croot/notebook-shim_1668160584892/work\r\n",
      "numpy==1.24.2\r\n",
      "oauthlib==3.2.2\r\n",
      "opt-einsum==3.3.0\r\n",
      "packaging @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_bet5qdixgt/croot/packaging_1671697440883/work\r\n",
      "pandas==1.5.3\r\n",
      "pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work\r\n",
      "parso @ file:///opt/conda/conda-bld/parso_1641458642106/work\r\n",
      "pathlib @ file:///Users/ktietz/demo/mc3/conda-bld/pathlib_1629713961906/work\r\n",
      "pexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\r\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\r\n",
      "Pillow==9.4.0\r\n",
      "pkginfo @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_9aeuxsywo9/croot/pkginfo_1666725052115/work\r\n",
      "platformdirs @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_7fs8_2xgrm/croots/recipe/platformdirs_1662711383474/work\r\n",
      "pluggy @ file:///opt/concourse/worker/volumes/live/42ef51ae-f79c-40e0-6d26-eb6fe9d8ba6e/volume/pluggy_1648042596777/work\r\n",
      "ply==3.11\r\n",
      "prometheus-client @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_19kjbndib7/croots/recipe/prometheus_client_1659455105394/work\r\n",
      "prompt-toolkit @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_82emz7mook/croot/prompt-toolkit_1672387300396/work\r\n",
      "protobuf==3.19.6\r\n",
      "psutil @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_c9b604bf-685f-47f6-8304-238e4e70557e1o7mmsot/croots/recipe/psutil_1656431274701/work\r\n",
      "psycopg2 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_973rh8dnaq/croots/recipe/psycopg2_1662147652256/work\r\n",
      "ptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\r\n",
      "pure-eval @ file:///opt/conda/conda-bld/pure_eval_1646925070566/work\r\n",
      "py4j==0.10.9.5\r\n",
      "pyasn1==0.4.8\r\n",
      "pyasn1-modules==0.2.8\r\n",
      "pycosat @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_ebsmghz9nu/croot/pycosat_1666805511853/work\r\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\r\n",
      "Pygments @ file:///opt/conda/conda-bld/pygments_1644249106324/work\r\n",
      "PyJWT @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_eec47dd9-fcc8-4e06-bbca-6138d11dbbcch6zasfc4/croots/recipe/pyjwt_1657544589510/work\r\n",
      "pyOpenSSL @ file:///opt/conda/conda-bld/pyopenssl_1643788558760/work\r\n",
      "pyparsing==3.0.9\r\n",
      "PyQt5-sip==12.11.0\r\n",
      "pyrsistent @ file:///opt/concourse/worker/volumes/live/76cffa60-bd33-4155-4e83-ea03c38b1294/volume/pyrsistent_1636111020441/work\r\n",
      "PySocks @ file:///opt/concourse/worker/volumes/live/112288ac-9cb0-4e73-768b-13baf4ca6419/volume/pysocks_1605305820043/work\r\n",
      "pyspark==3.3.2\r\n",
      "python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\r\n",
      "pytz @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_ddzpsmm2_f/croot/pytz_1671697430473/work\r\n",
      "PyYAML @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_79xo15pf1i/croot/pyyaml_1670514753622/work\r\n",
      "pyzmq @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_15f7a459-ad98-422b-b8da-cbf1f626e2115nt0ocwy/croots/recipe/pyzmq_1657724193704/work\r\n",
      "qtconsole==5.4.0\r\n",
      "QtPy @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_4e5ppuhz0f/croots/recipe/qtpy_1662014536017/work\r\n",
      "requests @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_516b78ce-034d-4395-b9b5-1d78c2847384qtnol99l/croots/recipe/requests_1657734628886/work\r\n",
      "requests-oauthlib==1.3.1\r\n",
      "rsa==4.9\r\n",
      "ruamel.yaml @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_7de1zqcaou/croot/ruamel.yaml_1666304553877/work\r\n",
      "ruamel.yaml.clib @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_c7s0zxy4t2/croot/ruamel.yaml.clib_1666302244557/work\r\n",
      "scikit-learn @ file:///opt/concourse/worker/volumes/live/cffa5efe-beec-4a37-5774-350794e43990/volume/scikit-learn_1642617139916/work\r\n",
      "scipy @ file:///opt/concourse/worker/volumes/live/9284487f-601d-4bc3-5556-535c4949d341/volume/scipy_1641557769615/work\r\n",
      "Send2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work\r\n",
      "sip @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_88z1zrsfrf/croots/recipe/sip_1659012373083/work\r\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\r\n",
      "sniffio @ file:///opt/concourse/worker/volumes/live/38ca9e9e-09d1-4d43-5a0f-b546422e7807/volume/sniffio_1614030472707/work\r\n",
      "soupsieve @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_14fb2zs6e3/croot/soupsieve_1666296397588/work\r\n",
      "spark==0.2.1\r\n",
      "sparkdl==0.2.2\r\n",
      "sqlparse==0.4.3\r\n",
      "stack-data @ file:///opt/conda/conda-bld/stack_data_1646927590127/work\r\n",
      "tensorboard==2.11.2\r\n",
      "tensorboard-data-server==0.6.1\r\n",
      "tensorboard-plugin-wit==1.8.1\r\n",
      "tensorflow==2.11.0\r\n",
      "tensorflow-estimator==2.11.0\r\n",
      "tensorflow-io-gcs-filesystem==0.31.0\r\n",
      "tensorflowonspark==2.2.5\r\n",
      "tensorframes==0.2.9\r\n",
      "termcolor==2.2.0\r\n",
      "terminado @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_18_p3gbeio/croot/terminado_1671751835656/work\r\n",
      "threadpoolctl @ file:///Users/ktietz/demo/mc3/conda-bld/threadpoolctl_1629802263681/work\r\n",
      "tinycss2 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_56dshjmms6/croot/tinycss2_1668168824483/work\r\n",
      "toml @ file:///tmp/build/80754af9/toml_1616166611790/work\r\n",
      "tomli @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_90762ba4-f339-47e8-bd29-416854a59b233d27hku_/croots/recipe/tomli_1657175507767/work\r\n",
      "toolz @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_a7gkswah88/croot/toolz_1667464082910/work\r\n",
      "tornado @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_1fimz6o0gc/croots/recipe/tornado_1662061695695/work\r\n",
      "tqdm @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2adqcbsqqd/croots/recipe/tqdm_1664392689227/work\r\n",
      "traitlets @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_0dtilxc0bw/croot/traitlets_1671143889152/work\r\n",
      "typing_extensions @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_4b7xacf029/croot/typing_extensions_1669923792404/work\r\n",
      "ujson @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_cf44fbd5-5db0-48cf-86c4-c8d4e74d1cbbwhgckc99/croots/recipe/ujson_1657544919410/work\r\n",
      "urllib3 @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_4cyi6_u2vj/croot/urllib3_1673575523861/work\r\n",
      "virtualenv==20.21.0\r\n",
      "voila==0.4.0\r\n",
      "wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client @ file:///opt/concourse/worker/volumes/live/5baed9cd-40fb-4fbe-6721-9568cdd0f2d7/volume/websocket-client_1614804245073/work\r\n",
      "websockets==10.4\r\n",
      "Werkzeug==2.2.3\r\n",
      "widgetsnbextension==4.0.5\r\n",
      "wrapt==1.15.0\r\n",
      "xgboost==1.5.0\r\n",
      "zipp @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b71z79bye2/croot/zipp_1672387125902/work\r\n",
      "zstandard @ file:///private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_40ayfv1xn6/croot/zstandard_1677014126754/work\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9892db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
